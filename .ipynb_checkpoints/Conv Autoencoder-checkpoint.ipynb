{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D, BatchNormalization, LSTM, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers,layers\n",
    "import keras.backend as K\n",
    "import datetime\n",
    "import time\n",
    "import requests as req\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['Dataset/HandOutlines_TRAIN', 'Dataset/HandOutlines_TEST']\n",
    "full_train_data = pd.read_csv(data[0], header = None)\n",
    "full_test_data = pd.read_csv(data[1], header = None)\n",
    "\n",
    "train_dataset = full_train_data.iloc[:,range(1,full_train_data.shape[1])] #traindatawithoutlabel\n",
    "label_test = full_train_data.iloc[:,0] #trainlabel\n",
    "\n",
    "test_dataset = full_test_data.iloc[:,range(1,full_test_data.shape[1])] #testdatawithoutlabel\n",
    "label_train = full_test_data.iloc[:,0] #testlabel\n",
    "\n",
    "train_dataset = train_dataset.values\n",
    "label_train = label_train.values\n",
    "\n",
    "test_dataset=test_dataset.values\n",
    "label_test=label_test.values\n",
    "\n",
    "test_data = np.expand_dims(test_dataset, axis=2)\n",
    "train_dataset = np.expand_dims(train_dataset, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('Orig_wafer.pkl', 'wb')\n",
    "\n",
    "orig_info = {}\n",
    "orig_info['X_train'] = np.squeeze(test_data)\n",
    "orig_info['y_train'] = label_train\n",
    "orig_info['X_test'] = np.squeeze(train_dataset)\n",
    "orig_info['y_test'] = label_test\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump(orig_info, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 32\n",
    "window_length = train_dataset.shape[1]\n",
    "epochs = 15\n",
    "test_samples = train_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 2709, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 2709, 32)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 387, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 387, 1)            97        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 43, 1)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 43, 1)             2         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_5 (UpSampling1 (None, 387, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 387, 32)           64        \n",
      "_________________________________________________________________\n",
      "up_sampling1d_6 (UpSampling1 (None, 2709, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 2709, 1)           65        \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1000 samples, validate on 370 samples\n",
      "Epoch 1/120\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 0.6931 - val_loss: 0.6926\n",
      "Epoch 2/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6926 - val_loss: 0.6921\n",
      "Epoch 3/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6921 - val_loss: 0.6916\n",
      "Epoch 4/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6916 - val_loss: 0.6911\n",
      "Epoch 5/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6911 - val_loss: 0.6907\n",
      "Epoch 6/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6907 - val_loss: 0.6902\n",
      "Epoch 7/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6902 - val_loss: 0.6897\n",
      "Epoch 8/120\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.6897 - val_loss: 0.6892\n",
      "Epoch 9/120\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6892 - val_loss: 0.6887\n",
      "Epoch 10/120\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.6887 - val_loss: 0.6882\n",
      "Epoch 11/120\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.6882 - val_loss: 0.6877\n",
      "Epoch 12/120\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.6877 - val_loss: 0.6872\n",
      "Epoch 13/120\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6872 - val_loss: 0.6867\n",
      "Epoch 14/120\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.6867 - val_loss: 0.6862\n",
      "Epoch 15/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6862 - val_loss: 0.6857\n",
      "Epoch 16/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6857 - val_loss: 0.6852\n",
      "Epoch 17/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6852 - val_loss: 0.6847\n",
      "Epoch 18/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6847 - val_loss: 0.6842\n",
      "Epoch 19/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6842 - val_loss: 0.6837\n",
      "Epoch 20/120\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6837 - val_loss: 0.6832\n",
      "Epoch 21/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6832 - val_loss: 0.6827\n",
      "Epoch 22/120\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6827 - val_loss: 0.6822\n",
      "Epoch 23/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6822 - val_loss: 0.6817\n",
      "Epoch 24/120\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6817 - val_loss: 0.6812\n",
      "Epoch 25/120\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.6812 - val_loss: 0.6807\n",
      "Epoch 26/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6807 - val_loss: 0.6802\n",
      "Epoch 27/120\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6802 - val_loss: 0.6798\n",
      "Epoch 28/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6798 - val_loss: 0.6793\n",
      "Epoch 29/120\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6793 - val_loss: 0.6788\n",
      "Epoch 30/120\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6788 - val_loss: 0.6783\n",
      "Epoch 31/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6783 - val_loss: 0.6778\n",
      "Epoch 32/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6778 - val_loss: 0.6773\n",
      "Epoch 33/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6773 - val_loss: 0.6768\n",
      "Epoch 34/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6768 - val_loss: 0.6763\n",
      "Epoch 35/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6763 - val_loss: 0.6758\n",
      "Epoch 36/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6758 - val_loss: 0.6753\n",
      "Epoch 37/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6753 - val_loss: 0.6749\n",
      "Epoch 38/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6749 - val_loss: 0.6744\n",
      "Epoch 39/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6744 - val_loss: 0.6739\n",
      "Epoch 40/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6739 - val_loss: 0.6734\n",
      "Epoch 41/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6734 - val_loss: 0.6729\n",
      "Epoch 42/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6729 - val_loss: 0.6724\n",
      "Epoch 43/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6724 - val_loss: 0.6719\n",
      "Epoch 44/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6719 - val_loss: 0.6714\n",
      "Epoch 45/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6714 - val_loss: 0.6710\n",
      "Epoch 46/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6710 - val_loss: 0.6705\n",
      "Epoch 47/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6705 - val_loss: 0.6700\n",
      "Epoch 48/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6700 - val_loss: 0.6695\n",
      "Epoch 49/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6695 - val_loss: 0.6690\n",
      "Epoch 50/120\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.6690 - val_loss: 0.6685\n",
      "Epoch 51/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6685 - val_loss: 0.6680\n",
      "Epoch 52/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6680 - val_loss: 0.6676\n",
      "Epoch 53/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6676 - val_loss: 0.6671\n",
      "Epoch 54/120\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6671 - val_loss: 0.6666\n",
      "Epoch 55/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6666 - val_loss: 0.6661\n",
      "Epoch 56/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6661 - val_loss: 0.6656\n",
      "Epoch 57/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6656 - val_loss: 0.6652\n",
      "Epoch 58/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6652 - val_loss: 0.6647\n",
      "Epoch 59/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6647 - val_loss: 0.6642\n",
      "Epoch 60/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6642 - val_loss: 0.6637\n",
      "Epoch 61/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6637 - val_loss: 0.6632\n",
      "Epoch 62/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6632 - val_loss: 0.6628\n",
      "Epoch 63/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6628 - val_loss: 0.6623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6623 - val_loss: 0.6618\n",
      "Epoch 65/120\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6618 - val_loss: 0.6613\n",
      "Epoch 66/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6613 - val_loss: 0.6608\n",
      "Epoch 67/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6608 - val_loss: 0.6604\n",
      "Epoch 68/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6604 - val_loss: 0.6599\n",
      "Epoch 69/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6599 - val_loss: 0.6594\n",
      "Epoch 70/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6594 - val_loss: 0.6589\n",
      "Epoch 71/120\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6589 - val_loss: 0.6585\n",
      "Epoch 72/120\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6585 - val_loss: 0.6580\n",
      "Epoch 73/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6580 - val_loss: 0.6575\n",
      "Epoch 74/120\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.6575 - val_loss: 0.6570\n",
      "Epoch 75/120\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.6570 - val_loss: 0.6566\n",
      "Epoch 76/120\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.6566 - val_loss: 0.6561\n",
      "Epoch 77/120\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6561 - val_loss: 0.6556\n",
      "Epoch 78/120\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.6556 - val_loss: 0.6551\n",
      "Epoch 79/120\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.6551 - val_loss: 0.6547\n",
      "Epoch 80/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6547 - val_loss: 0.6542\n",
      "Epoch 81/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6542 - val_loss: 0.6537\n",
      "Epoch 82/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6537 - val_loss: 0.6532\n",
      "Epoch 83/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6532 - val_loss: 0.6528\n",
      "Epoch 84/120\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6528 - val_loss: 0.6523\n",
      "Epoch 85/120\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6523 - val_loss: 0.6518\n",
      "Epoch 86/120\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6518 - val_loss: 0.6514\n",
      "Epoch 87/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6514 - val_loss: 0.6509\n",
      "Epoch 88/120\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6509 - val_loss: 0.6504\n",
      "Epoch 89/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6504 - val_loss: 0.6499\n",
      "Epoch 90/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6499 - val_loss: 0.6495\n",
      "Epoch 91/120\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.6495 - val_loss: 0.6490\n",
      "Epoch 92/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6490 - val_loss: 0.6485\n",
      "Epoch 93/120\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6485 - val_loss: 0.6481\n",
      "Epoch 94/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6481 - val_loss: 0.6476\n",
      "Epoch 95/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6476 - val_loss: 0.6471\n",
      "Epoch 96/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6471 - val_loss: 0.6467\n",
      "Epoch 97/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6467 - val_loss: 0.6462\n",
      "Epoch 98/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6462 - val_loss: 0.6457\n",
      "Epoch 99/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6457 - val_loss: 0.6453\n",
      "Epoch 100/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6453 - val_loss: 0.6448\n",
      "Epoch 101/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6448 - val_loss: 0.6443\n",
      "Epoch 102/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6443 - val_loss: 0.6439\n",
      "Epoch 103/120\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6439 - val_loss: 0.6434\n",
      "Epoch 104/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6434 - val_loss: 0.6429\n",
      "Epoch 105/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6429 - val_loss: 0.6425\n",
      "Epoch 106/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6425 - val_loss: 0.6420\n",
      "Epoch 107/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6420 - val_loss: 0.6416\n",
      "Epoch 108/120\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6416 - val_loss: 0.6411\n",
      "Epoch 109/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6411 - val_loss: 0.6406\n",
      "Epoch 110/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6406 - val_loss: 0.6402\n",
      "Epoch 111/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6402 - val_loss: 0.6397\n",
      "Epoch 112/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6397 - val_loss: 0.6392\n",
      "Epoch 113/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6392 - val_loss: 0.6388\n",
      "Epoch 114/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6388 - val_loss: 0.6383\n",
      "Epoch 115/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6383 - val_loss: 0.6379\n",
      "Epoch 116/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6379 - val_loss: 0.6374\n",
      "Epoch 117/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6374 - val_loss: 0.6369\n",
      "Epoch 118/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6369 - val_loss: 0.6365\n",
      "Epoch 119/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6365 - val_loss: 0.6360\n",
      "Epoch 120/120\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 0.6360 - val_loss: 0.6356\n"
     ]
    }
   ],
   "source": [
    "input_window = Input(shape=(window_length,1))\n",
    "x = Conv1D(32, 7, activation=\"relu\", padding=\"same\")(input_window) # 10 dims\n",
    "#x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(7, padding=\"same\")(x) # 5 dims\n",
    "x = Conv1D(1, 3, activation=\"relu\", padding=\"same\")(x) # 5 dims\n",
    "#x = BatchNormalization()(x)\n",
    "encoded = MaxPooling1D(9, padding=\"same\")(x) # 3 dims\n",
    "\n",
    "encoder = Model(input_window, encoded)\n",
    "\n",
    "# 3 dimensions in the encoded layer\n",
    "\n",
    "x = Conv1D(1, 1, activation=\"relu\", padding=\"same\")(encoded) # 3 dims\n",
    "#x = BatchNormalization()(x)\n",
    "x = UpSampling1D(9)(x) # 6 dims\n",
    "x = Conv1D(32, 1, activation='relu')(x) # 5 dims\n",
    "#x = BatchNormalization()(x)\n",
    "x = UpSampling1D(7)(x) # 10 dims\n",
    "decoded = Conv1D(1, 2, activation='sigmoid', padding='same')(x) # 10 dims\n",
    "autoencoder = Model(input_window, decoded)\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "history = autoencoder.fit(test_data, test_data,\n",
    "                epochs=120,\n",
    "                batch_size=1024,\n",
    "                shuffle=True,\n",
    "                validation_data=(train_dataset, train_dataset))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "decoded_stocks = autoencoder.predict(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv Auto Encoder Reconstruction error : 0.36162702142010433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 343, 1) (36, 343, 1)\n",
      "0.4812057950921338\n"
     ]
    }
   ],
   "source": [
    "print(decoded_stocks.shape,train_dataset.shape)\n",
    "# print(np.absolute(x_test_deep[0]))\n",
    "# print(np.absolute(decoded_stocks[0]))\n",
    "# print(len(x_test_deep[0]))\n",
    "def repr_error(actual, predicted):\n",
    "    final_err = 0\n",
    "    for i in range(len(actual)):\n",
    "        err = np.sum(np.absolute(np.absolute(actual[i]) - np.absolute(predicted[i])))\n",
    "        err = err / len(actual[i])\n",
    "        final_err += err\n",
    "    final_err = final_err / len(actual)\n",
    "    return final_err\n",
    "# final_test = np.sum(np.absolute(np.absolute(x_test_deep[0]) - np.absolute(decoded_stocks[0])))\n",
    "# print(final_test/150)\n",
    "print(repr_error(train_dataset, decoded_stocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# open a file, where you ant to store the data\n",
    "file = open('Conv_wafer.pkl', 'wb')\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump(autoencoder, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.layers[4].name = \"embed_0\"\n",
    "layer_output=autoencoder.get_layer('embed_0').output\n",
    "intermediate_model=Model(inputs=autoencoder.input,outputs=layer_output)\n",
    "intermediate_prediction=intermediate_model.predict(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_test = np.squeeze(intermediate_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_prediction=intermediate_model.predict(test_data)\n",
    "latent_train = np.squeeze(intermediate_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('Latent_toesegmentation2.pkl', 'wb')\n",
    "\n",
    "latent_info = {}\n",
    "latent_info['X_train'] = latent_train\n",
    "latent_info['y_train'] = label_train\n",
    "latent_info['X_test'] = latent_test\n",
    "latent_info['y_test'] = label_test\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump(latent_info, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(343, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xU15338c+ZkTSj3hsqqCBAgAFhDDZY2I6747iQYscpjmPHSTabV9qT3XjzbJ7sbnovTjFeJ/Em2SR2AokTd+MCLtgIhCmWUAHUUBn1Lo1mzvOHJEyRhMrM3Dt3fu/Xi5fFaObOzxfdr86ce4rSWiOEECL42YwuQAghhG9IoAshhEVIoAshhEVIoAshhEVIoAshhEWEGfGmKSkpOi8vz4i3FkKIoLVv3752rXXqdN83JNDz8vIoKysz4q2FECJoKaXqZvq+dLkIIYRFSKALIYRFSKALIYRFSKALIYRFSKALIYRFSKALIYRFSKALIYRFGDIOXYjTjY55+d/X6+gcGCU1zskdG3Kx25TRZQkRdCTQheEeevk4336qEqVAa3j9WAc/fX8JSkmoCzEX0uUiDHW0pY+fv1DDlcvTOP7Nd/L5q5fyj4PN7KxoM7o0IYKOBLowzNCohzse3EOUw86/37gCgE9eXkhBSjTff7YK2U1LiLmRQBeGeeatFjoGRvnh+9aSlxINQLjdxl2b86ho7uWt5l6DKxQiuCw40JVSOUqpF5RSFUqpI0qpz/iiMGF9O8qbWBTv5OKC5DMev3H1IsLtih37mwyqTIjg5IsW+hjwBa11MXAx8Cml1AofHFdYWFvfMLuqXNxSkoXtrBEtidERlBal8mxFq0HVCSv4zSvHuf7Hu/ne00fxekOj+27Bga61btZa75/4ug+oALIWelxhbY8dOIlXw9Z1U/+oXLY0lbqOQeo6BgJcmbCC4+0DfO3xCnqH3Nz/Qg0/e6HG6JICwqd96EqpPKAEeH2K792rlCpTSpW5XC5fvq0IQn8/2MwFWfEsSYud8vulRSkA7K5uD2RZwiK++3QlEWE2dnxqE5ctTeV/36gPiVa6zwJdKRUD/AX4rNb6nLtZWuttWuv1Wuv1qanTbrghQkDXwCgHG7u5qjh92ufkp0STlRDJ7mr55S/mZn99F08cauHeLQWkxTp5z4XZNPcM89qxDqNL8zufBLpSKpzxMP+91nq7L44prOuV2na0htKlKdM+RynFlqUpvFrTwZjHG8DqRLD79SsniI8M52OlBQBcvSKdiDAbL1Raf26DL0a5KOAhoEJr/YOFlySs7qWjLuKcYazOip/xeaVFqfSNjPFmY3eAKhPBrm/YzTNHWnjXmkyiHeMT4Z3hdi7IiudAg/V/jnzRQt8MfAh4h1LqwMSfG3xwXGFBw24PTx1p4aoV6YTZZ/7x21SYjFLwcrX1PyoL39i+v4mRMS+3lmSf8XhJTgKHmnoYHbP2pz1fjHJ5WWuttNartdZrJ/484YvihPXsrGijb3iMrWddcFNJiIpgWXos++u7AlCZCHb9I2P89PlqNuYnsS434YzvleQmMjLmpbLF2pPVZKaoCKinjrSQEuPgksLk8z8ZKMlN4EBDd0iMUBAL87cDTbT3j/Iv1y0/Z2G31dnj3XuHmyTQhfAJr1fzcrWLLUtTZr08bklOIj1Dbo7LeHRxHtv3N7E0Peac1jlAVkIkznAbta5+AyoLHAl0ETCHT/bQNehmS9Hsh62WTFyc5fXWv6El5u9oSx/76rq4tSR7ymWXbTZFQUqMBLoQvrKzog2lYPOS6Ycrnq0wNYZYRxjl0o8uZvCdpyqJdYZx+0U50z6nME0CXQif0Frz1wNNXFKQTGqsY9avs9kUa3MTpIUuplXd2sfOyjY+vqWAxOiIaZ9XmBpNY9cQw25PAKsLLAl0ERD767uo6xjk1pK5L/NTkptIZUsvg6NjfqhMBLvt5U3YbYr3zdA6h/FPe1rDMZd178dIoIuA2L6/CWe4jesvyJzza0tyE/BqONjY44fKRDAbGfOwY38TpUUppMU6Z3xu/sSa+1Ze8E0CXfjdyJiHfxxs5tqVGcQ45r6NbXFGHADVbdbu/xRz99vX6mjpHebuS/PP+9zc5CgA6joH/V2WYSTQhd+9UNlGz5B7Xt0tAOlxDqIj7NRKoIvT9Ay6+enzNWxZmkrpLEZOxTnDSYqOoK5DAl2Iedu+v4nUWAeXzmF0y+mUUiExQkHMzc9frKF32M2Xrls+69fkJkVR3yldLkLMS9fAKC8cbePmNYvOu3bLTApTYyx9M0vMTWPXIL9+9QRbS7JZsShu1q9bnBwlLXQh5usfh5pxezS3TrMz0WwVpkbT1D0kI10EAL94sRaAL1yzdE6vW5wUxcnuIcsu0iWBLvxq+/5GlqXHsiJz9q2oqRSmxgDWHnImZmfY7eGxN09y4+pMFiVEzum1ucnReDWc7B7yU3XGkkAXftPaO0x5fTc3rV005XTsuShMGw906UcXz1W0znrFzrNlxo8PbWzuGfZ1WaYggS78ZnI/0CuWpS34WIuTo7ApqJUWekjzeDX3P1/D4uSoWa/Yebr0uPFAb+2VQBdiTnZXu0iJcbA8Y+qNoOfCEWYnNylKWugh7vFDzVS29PHFa5fNesXO02VMtNBbJNCFmL3RMS+7qlyUFqVgm8eFN5XC1BgZix7iHi1rIDsxkhtWzX3GMUCMI4wYRxgt0uUixOy9VOWia9DNu9bM78KbSmFaDMfbB/DIZhchqbV3mFdq2rm1JGtBjYT0OId0uQgxFzvKG0mOjpjVDL7Zyk+JZmTMS3OPNUcoiJntrm7Hq+GdqxfWSMiId0qXixCz5fZ4eemoi2tXZRC+gMlEZ8tNGl+Lo6FTAj0Uldd3EesIY2nawu7JpMc5aZUuFyFmp7y+m4FRD1uK5jfVfzpvB7p1Z/qJ6ZXXd7M2N2HB92Qy4py09Y1Ycp9aCXThc7urXdgUXFLo20DPjHditynqJdBDzuDoGJUtvZTknLtf6FxlxjsZ82raB0Z8UJm5SKALn9Ja89ThFi5cnEh8ZLhPjx1mt7EowUlDlwR6qDnY2INXj292slCTY9GtONJFAl341OGmXqrb+rl57cLWbplOTmKUtNBD0OQWhGt90EI/NRZdAl2ImW0vbyTCbuPGBY5EmE5uUpTcFA1BBxq6yE+JnnHP0NnKsPBsUQl04TNuj5e/v3mSdyxPIyFq4RfeVHKSomjvH5FVF0OI1pr99d0+6T8HSI5xYLcpSw5dlEAXPrOrykV7/yhbF7hU7kxyZOhiyGntHcHVN8IaHwW63aZIi3XQ0iM3RYWYktaanz5fQ0ack8t9sBjXdGToYuipaO4FmNNGFueTHueULhchpvN8ZRsHGrr5/DVLiQjz34/VZKDLjdHQ8dZEoC/zwSJvkzLirDlbVAJd+MQjZQ2kxDjYOs+NoGcrMSqc6Ai7DF0MIRXNvWQnRhLn9N0w2Ix4a84WlUAXC9Y9OMrzlW3cvHZh+4bOhlKKnKQo6XIJIZUtfRQvcMers6XHOekbGWNgxFo31yXQxYJt23UMt0fz3vVz30FmPnKSZCx6qBh2ezjm6vd5oGdadF10CXSxIHtPdPLQy8e5Ze0ilmf49qKbzuRYdK2ttxaHOFNVax9eDcU+7D8H684WlUAX89bSM8ydv3qDrIRI7ruhOGDvm5sUxZDbQ3v/aMDeUxijsrkPwOctdKvOFvVJoCulfqWUalNKHfbF8YT5aa357tNHGfNofnPXhlMtnkDISRrf6V1ujFrfW829REXYT41u8pXJ2aLS5TK13wDX+ehYIgj88Nkq/rK/kbtL88lN9u3Fdj4yFj10VDT3siwj1mfbGE6KjLAT5wyz3Fh0nwS61noX0OmLYwnzq27t4/4XathaksUXr1kW8PfPTpwYi94hgW51NW39LEv3bf/5pIx4p3S5zJdS6l6lVJlSqszlcgXqbYWPdQ2M8oVH3yQ6Ioz/e+MKn7ecZsMZbict1iFdLhbXNTBKx8AoS9Ji/HJ8K84WDViga623aa3Xa63Xp6b6bp9JEVhff6KCypY+fnjbWpJ8sPLdfOXIqouWd6y9H4DCVP8EelqsE1eftdZzkVEuYtYGR8d44lAzW0uyuGpFuqG1ZMRbr3UlzlTT5t9AT4114OofsdTwVwl0MWuPH2xmcNTD1nWBmUA0k/TY8bU4rHQxijPVugaICLORlRjpl+OnxjpwezQ9Q26/HN8Ivhq2+AfgNWCZUqpRKXW3L44rzGNkzMNPnq9meUYs6xcvfBuwhcqIdzA46qHfYlO3xdtq2/opSInG7qf7NKmxDgBLdbuE+eIgWuv3++I4wryeONRMQ+cQv77rIkNuhJ4t/bRdZ2J9uGiTMI9aVz8rF8X77fipMW8HepGfRtIEmnS5iFl58aiLlJgILisyxw3tt6duW6d1Jd42MuahvnOQwtRov73HqRZ6v3V+hiTQxXl5vZqXq9u5dEmKKVrnYO19IQXUdQzi1VDopyGLYM0uFwl0cV5vNffSMTDKlqXmaJ3DaS10CXRLqvXzCBeAOGcYEWE2CXQRWnZXtwNw6ZIUgyt5m1Wnbotxta7xQM9P8V+Xi1KK1BiHBLoILburXSzPiCUtgAtwzYaMRbeu6rZ+FsU7iXb4ZNzGtCbHoluFBLqY0eDoGGUnukzV3TIpPc5JS691LkbxtsrmPpb7eMncqaTGSgtdhJDnKtoY9Xi5Ylma0aWcIz3OmvtChrqRMQ+1rn6W+3hTi6lIoIuQsn1/I4vinWzMTzK6lHNkxDlx9Y/g8cpsUSupbu1nzKt9vqnFVFJjHHQOjuL2eP3+XoEggS6m1TPoZnd1OzeXZJlmuOLp0uOdeLyaDgv1gYrxTaHB97sUTSU11oHW0Dlgjd2vJNDFtF6pbcfj1VxVbL7uFoD0iXHEMnTRWg439eAMt/l1hMskq41Fl0AX09pd7SLWEcaa7ASjS5nS5L6QrXJj1FLKG7pZnZ3gtzVcTieBLkKC1ppdVe1sWpJMmN2cPyYyuch6ht0e3jrZQ0luYBoRp6/nYgXmvFKF4Y63D9DUPUSpSdZumUpKjAO7TclIFws5crIXt0ezLjcwK3pabT0XCXQxpcnZoVtMHOh2myIlJoK2Pgl0q3ilZvznLlAtdGe4nVhnmLTQhbXtqnKxODmK3OQoo0uZUVqsU/rQLUJrzY7yJi4uSCItNnCzkq00Fl0CXZxjcriiGScTnS09zkGbRS7GUFdW18Xx9gG2lgR2RywrrecigS7O8fihZkY9Xt5tgq3mzic11kmb3BQNelprvvvUUVJiHLxzdWZA39tK67lIoItzPH7oJIWp0azK8v/EjoVKj3PQMWCdmX6h6rmKNt440cnnri7y+4JcZ5MuF2FZg6Nj7D3exZXF6ShlvtmhZ5vsa7XKBRmK+obdfPPJCgpSo7ltfU7A3z811kH/yBiDo8G/P60EujjD68c7GfV4KS0yz9rnM0mPGx92Jv3owUdrTXPPEO9/cA/1HYN89V0rDZnzMDkWvb0v+Kf/B/azjTC93VXtOMJsXJRnvsW4pjLZQpd10YPPp/9Qzj8ONuMMt/HgnesNW6L57bHow6Yf1XU+EujiDLurXWzIT8IZbje6lFmZbKFLoAeXXVUu/nGwma3rsvhYaUFAFuKajpWm/0ugi1Oae4aobuvnfQb0Y85XSoyDiDAbjV1DRpciZsnj1XzzyUpykiL55tYLcIQZ23iwUqBLH7o4ZXJ2aOnS4Og/B7DZFNkJkTR0Dhpdipilv5Y3UdHcyxevXW54mAMkRzsIsymaLbCEhAS6OGV3dTupsQ6Wpft/pxhfyk6KoqFLAj0YDLs9fP+Zo6zOjufGCwI73nw6dptiUUIkDRb4lCeBLgDwejUvV7soLUoJiuGKp8tJjKShM/gvxlDwaFkDJ3uG+dL1y021aUpOkjU+5UmgC2B8lbuuQbepF+OaTk5SFD1DbnqH3UaXIs7jz/saKc6MY1Ohubr1chKjaLTApzwJdAHArmoXAJuXmOtCm43cpPGhZlZoYVlZraufNxt72FqSZXQp58hJiqK9fzToJxdJoAtgfLjiisy4U3f8g0lO4mSgS7eLme2saAXgxjXm6Ds/XXZiJEDQj5aSQBcMjIyxr64rqEa3nC4nafJilBa6me2ubmdpegyZ8ZFGl3KO7ERrfMqTQBe8frwDt0cHZf85QHxkOLGOsKBvXVnZsNvD68c7TbsD1mSjQAJdBL1dVe04w21cuDgw2375mlJqfOhikF+MVra/vovRMS+blyQbXcqUUmMcOMNtQT90UQJdsLvaxcb85KCZ7j+VnMRIGYtuYuX13QCU5Jiz0aCUIjtx9o0Ct8eL16v9XNXcSaCHuOaeIWpdA0GzuuJ0cpKiaOgcQmvzXWRiPNALUqJJjI4wupRpjTcKZm6he72a//z7Wyz9v09y2fdeoLq1L0DVzY4EeojbV9cFwMZ8c34Unq2cxEiG3B46BoJ/CVSr0VpzoKGLtQHa+Hm+cpKiaOwcnLZRMObx8tk/HeBXrxznlrVZDLu9vPeB19hf3xXgSqfnk0BXSl2nlDqqlKpRSn3JF8cUgVFe340jzMbyzOCa7n+2xSnRANS29RtciThbS+8w7f2jrMk2eaAnRtE3Mkbv0NRj0X+3p47H3jzJv1y3jB+8bw1/+cQm4iPD+cCDr/Pi0bYAVzu1BQe6UsoO/Ay4HlgBvF8ptWKhxxWBUV7fxerseMIN2FjAl1ZnxQNwoKHb4ErE2WrbBgAoSo8xuJKZTY50qZ+iH7132M1Pnq9hU2Eyn7ysEKUUuclR/PkTm8hPieaeh8t4wQSh7oureANQo7U+prUeBf4I3OyD4xpm2O3hePsAdR0Dlu6THXZ7OHyyl7U55m45zUZyjIPFyVGnbr4J86h1jX9qWpJq7kBfkjb+KbWiufec7z3wUi2dA6Pcd33xGWsdpcY6+OPHL6YgNZqvPnaE0TFj97b1RaBnAQ2n/b1x4rEzKKXuVUqVKaXKXC6XD97W97oHR/nbgSau+sFLXPG9F7nsuy9y12/28nxlqyU3Id5XNz6U7OKC4O4/n1SSk8D++i5L/xIORrWufmIdYaafhVyQEk2cM4zyhjP7xHuG3Dz08nFuWrOIC7Ljz3ldnDOc+24opq5jkH/bcYgxA7PCFxtcTLVk2jlXlNZ6G7ANYP369QG74vqG3fxuTz0DI+dfo+FvbzbR0DlEcnQE39p6AW19I/zouSpePOpiU2Ey163K4I4NuYbse+gPu6pdhNuVZQJ93eJE/nrgJA2dQ0G/lZiV1Lr6KUiLMf0qnjaboiQ38ZxPeU8cambY7eWe0vxpX3vFsjQ+c2URP95ZTUvPMNeuTOeOjYuxn7WipNbar+fBF4HeCJy+xU02cNIHx10wV98IH/n1Gxw52XvOiZ1KeqyDhz+6gXW5CcQ6wwG4/aIcHj/UzPeePsqrtR3sqmrn/jtKgnrM9qSXq9tZl5tItMMaG1dNLiy2u8bFB5IXG1yNmFTbNsAmk04oOltJbgI/3llN37D7VAb8ZV8jhanRXJB1buv8dJ+7einJMRF868lKXq5p509lDaTFOiktSuEjm/Jo6Bzi848c4HvvXUPexE18X/PFlbwXKFJK5QNNwO3AHT447rx19I/w2T8doLy+mzGvl1/fdRFXLEub17HS4pzctTmfuzbn8/CrJ/jq34+w8Rs7uao4na/fuipog729f4QjJ3v54rXLjC7FZwpSoslKiGR3VTsf2CiBbgYDI2O09A5T4KcA87VNhSn86Llqdla0cUtJFi9Xt1NW18W/37hiVi3rD1+Sx4cvyeN3e+p4pKyBuo4Bnq9s4+cv1tI/PIYj3Eb3kP+WeV5woGutx5RS/ww8DdiBX2mtjyy4snlq7Brkww+9QVP3ELeWZPH+Dbms8dFNvzs35ZGdGMlfD5zkL/sbaeoe5HNXLWXd4sSgGyXySs34dnPBun7LVJRSlBal8I+DzfQOu4mbaGEJ45zoGB/hkp9i7huik9YvTiQrIZLt5U3ctGYR33yygqyESD54ce6cjvPBixfzwYsX4/VqfvPqCY629BEepvjIprxTN1/9wSeftbXWTwBP+OJYC1HV2seHHnqdoVEPv7tnIxflJfn8Pa4sTufK4nSuKk7jC4+8yW3b9nBxQRK3rM1i67psIsKCI9h3VbWTGBXOykXG7bbuDx+8eDF/3NvAz16o4b7ri0893j8yxo79jQy7z7xhtWVpKssygnsMvpkdb58M9OBoodtsindfmM1Pdlbzyd/v48jJXn5029p5731qsyk+eun0fe++Zo3OU2BfXScf/U0ZjjAbj3ziEpZn+Deobl6bxbrcRHZWtPKNJyvZc6yTxw8188sPXmj6PunRMS/PV7ayZWmqqbYB84VVWfG858JsHnjpGHXtg0RGjF+Ih5t6qJ5i0tEPnq3inasz+cimPFadp49UzN2JiUDPSwmem9T/dHkhVS19PHWkhU2Fydy0ZpHRJc2auZNnll6obOOTv99HZnwk//PRDeQkBeaHJycpio9szuf9G3P5a3kT920/xBXfe5FLi1L4r5tXmTbYX6py0TXo5ua1wfODOhdfv3UVYTbFq7Udpx5zhtt48MPruaTw7Ztz3YOj3Lf9EE8faeHJQ8088KH1bF6SjFIKrTVnj3602i+/QDjePkh6nIOoCHNeC1Nxhtv55YcuZGBkjMhwe1D9uwfPWZ7G9v2NfPHPB1mRGcev77qIlJjAj3V1hNm57aJc0uKc/OH1ev5a3sT2/U1ctzKDz15dxNK0WFP9UPzxjXqSoyNMuzb1QjnC7Hzr3avP+7wYRxi/vXsjbb3DfPhXb/DBh14nMSqc/7x5FX94o/6MXwgRdhufv2YpN67OPLUZgji/4+39QdPdcjazNshmooyYhLF+/XpdVla24OP89+5jfO3xCjYvSeaBD60nxiT/AK/VdvD0kRYefu0EWsOVy9N4z4XZXLMyY1bDJ/1p74lO3vvL1/jC1Uv59JVFhtZiJj1Dbv60t55Hyxqpbusnwm7jrs15p1qW++u7eKnKhVLw1Xet5M5NecYWHCTW/dezXLsynW9uPf8vWHF+Sql9Wuv1033fHAk4R1prvv3UUX75Ui03XJDBDxdw08IfLilM5pLCZG67KIfn3mrlh89VsbOyjetWZvCj29caNtRRa803nqggLdbB3TNMkghF8ZHh3LulkDs2Lub1Yx3kpURTeNpUdY9Xs+dYB79+5QT/77EjdAyM8rmrikw/WcZIPUNuOgdGyUsOzhZ6MAqqQNda88CuYzx24CRvNffygY25/OfNqwxv9U6nODOO4sw4PnTJYv68r5GvPV7BDT/ezZalqdx3w/KA/xJ6pKyB8vpuvv3uC4KqTzOQYhxhXFmcfs7jdpti85IUNuYn8W87DvGTndV09I+Y+ufPaCeCbISLFQTVVf31xyv475ePsyorji/fUMw9pflB0UJKiIrgntICMuKd/Pa1On7z6gn+tLeBG1dn8ul3FDH5v6A1/OKlGv7xZjMASsHtG3K559J80uKcC6rht6+d4CuPHWFjfhLvXpe9wP+j0BVmt/Htd68mOcbBL16spWtw1HSfEM0i2IYsWkFQBfqVxelEOcKC9qPujasXcePqRTz7VitPHmrm0X2NPLqv8Zzn3bJ2EUnRDpp7hti26xjbdh3jw5cs5qY1i1g/x7H1Wmt+9Fw1P95ZzVXFadx/xzrLrEVjFKUU/3rdcpKjI/ja4xV0D+5l24fNcw/HLI63D6AUARt1JoL8pmiwe+N45zlrL2cnRp5aLEtrze7qdp441Mwf944vaLm1JIurV6Rz/QWZ5z2+x6v56mNH+O2eOt5zYTbf2nqBhLmPTY6yuv2iHL5+6wVGl2Mqn/ljOfvqunj5X99hdCmWYcmbolaxIT+JDfnTt7iVUmxZmkppUQofv6yQh189wf+8doLt5U1sXpJMdEQY712fw9Urzuzz9Xo1P3+xhmcr2nizoZuPX1bAl65bHpSfasxu67ps3mzo5nev1/PRS/PPuJEa6mra+imQ8xFQ0kIPMh6v5jtPVbKrup3uwVFaeodZFB95xnNGPV5cfSMsTY/hjg25fGSzjGjxp/b+ES7/7otsKkxm24enbTyFFLfHy8qvPM1dm/O474bi879AzIq00C3GblPcd0Mx9wFDox5++nw1rb0j5zzvorxEbrsoR1rlAZAS4+ATlxXwvWeq2Hui0y9rCAWb4+0DjHq8FGdaa60gs5NAD2KREXb+5brlRpchgLsvLeC3e+r4xhMV/PkTm0J+KOPkNm7Bvvl4sJE7ZEL4QGSEnS9eu5zy+m6+8MiBkN8G763mXiLsNrmnEGAS6EL4yHsuzOZTVxTy1wMnOXLy3I2GQ0llcx9L0mKCbp+AYCdnWwgf+lhpARF2G9v3NxldiqEqmnulu8UAEuhC+FBCVATvWJ7GjvJG+ob9t9WYmXX0j9DWN8IKuSEacBLoQvjYJy8vpGvQzbZdx4wuxRCVLX0AMsLFABLoQvjYmpwE3rVmEQ/uPkZr77DR5QTcqREusrVfwEmgC+EHX7xmGR6v5ofPVhldSsC91dxLWqyDZAM2mwl1EuhC+EFuchQfujiPR8oaqGrtM7qcgKps7pPuFoNIoAvhJ59+xxKiHWF8+8lKo0sJGLfHS01bvwS6QSTQhfCTxOgI/unyJeysbOO10/YntbJaV//ElH/pPzeCBLoQfnTX5jwWxTv55pMVeL3Wnz1a2SwjXIwkgS6EHznD7XzhmmUcbOzh8UPNRpfjd9VtfYTZlOxSZBAJdCH87JaSLIoz4/jO05WMjHmMLsevatsGWJwcJVP+DSJnXQg/s9sU912/nIbOIX6/p97ocvyq1tUvC3IZSAJdiACY3HnqJ89XU98xeP4XBKExj5cTHQMUpkmgG0UCXYgA+cqNKwB43wOv0TuPdV601tR1DFDd2kd1ax8DI2O+LnFBGrqGcHu0tNANJBtcCBEgRemx/OauDdzys1f45Yu1c9qcZMzj5d92HOKRssZTj6XEOHj4oxexclG8P8qds5q2fgAKU+WGqFGkhS5EAK3NSeDWkix++VItj5Y1zOo1w24Pn/z9fnDDUoYAAA3hSURBVB4pa+SeS/O5/44SfnjbGsLtitsf2MO+ui4/Vz07lc29KAVL02UMulGkhS5EgH391lW09g7z7387zJalqaTHOad9bu+wm3seLmPviU7+46aV3Lkp79T3NuYnc9u21/jin9/kmc9uIczgkSUVLb0sTooi2iGxYhRpoQsRYFERYXxr62o8Xs2XdxxidMx7znP6ht187H/KuOQbOymv7+LHt5ecEeYAixIi+cqNKznmGuBHz1Ubvu1dRXMfyzNkQpGRJNCFMEBuchRfvqGY5yrauPvhvWfc4HT1jXD7tj28UNnGO1dn8tu7N3LTmkVTHueq4jTevS6b+1+o4ZFZduH4w+DoGCc6BmSGqMEW9NlIKfVe4KtAMbBBa13mi6KECAUf2ZxPlCOM+7Yf4v0P7uG6VRloDY+WNdDSO8yDd67nimVpMx5DKcV337Oauo4BvvdMFTeuXmRIl0dNWz9awzJZA91QC22hHwa2Art8UIsQIed963N44IMXcsw1wHeeOsp3nz5K/8gYv7/n4vOG+SSbTXHfDcW4+kb4793H/Vzx1E5MjK2XKf/GWtCvcq11BYy3EoQQ83PVinQOfOVqxiYW7wq327Db5nZNXbg4ketXZfDArlo+dMlikqIj/FHqtOraBwDITYoK6PuKMwWsD10pda9SqkwpVeZyuQL1tkIEhTC7DWe4HWe4fc5hPunT7yhicNTD39886ePqzq+uc5C0WAeREfaAv7d423kDXSn1nFLq8BR/bp7LG2mtt2mt12ut16emps6/YiHElFYsiqM4M47t+xvP/2Qfq+8YZHGytM6Ndt4uF631VYEoRAixcFtLsvj6ExUBXySrrnOAS5dIQ81oMmxRCAu5ee0ibAp27G8K2HsOuz209o5IC90EFhToSqlblVKNwCXA40qpp31TlhBiPtLinFxalMqO8qaA7ZB0fOKGaJ6McDHcggJda71Da52ttXZordO11tf6qjAhxPxsLcmiqXuIN050BuT9al3ji3ItkVUWDSddLkJYzDUr04mOsAes26W2bQClZAy6GUigC2ExURFhXLMyg2feaglIt0utq5+shEgZsmgCEuhCWNDly1LpGnRz5GSv399Ltp0zDwl0ISxo85IUAHZV+3cSn9erOeYaoEA2tTAFCXQhLCglxsGKzDh2+znQ6zsHGXJ7WC6LcpmCBLoQFlW6NIV9dV1+3Xu0onm8S0eWzTUHCXQhLGpLUSpuj+b14x1+e4+K5l5ssu2caUigC2FRFy5OxBluY1dVu9/eo6Klj/yUaJzhMsLFDCTQhbAoZ7idjfnJfu1HP9rSx3LpbjENCXQhLKy0KIVa1wBN3UM+P7bb46Wpe4gCmVBkGhLoQljYlqXjKyC+7IdWelPXEB6vlk0tTEQCXQgLK0qLIT3Owa5q3/ej13WObzu3OFla6GYhgS6EhSmlKC1K5ZWadjw+XgagvmN8lUVZNtc8JNCFsLjSohS6B90cburx6XHrOgZxhttIi3X49Lhi/iTQhbC4SyeWAfD1aJe6zkFyk6Jkk3gTkUAXwuKSYxysyorzeT96fccguUnSf24mEuhChIDSolT213XRN+z2yfG01tR3ysbQZiOBLkQIKC1KYcyr2XPMN7sYufpGGHJ7JNBNRgJdiBBw4eJEIsPtPutHnxyyKGPQzUUCXYgQ4Aizc3FBErt91I9e1yFj0M1IAl2IEFFalMrx9gEauwYXfKz6jgFsCrISIn1QmfAVCXQhQsSG/CQA9tV1LfhYJzoGWZQQSUSYRIiZyL+GECFieUYsznAb5fXdCz7WsfZ+CmQfUdORQBciRITZbazOTqC8YWGB7vVqatsGKJR9RE1HAl2IEFKSm8BbJ3sYdnvmfYyW3mGG3B4KpYVuOhLoQoSQiwuScXs0e0/Mfzx6rasfQALdhCTQhQghG/OTiLDbFjR8sbZtItDTpMvFbCTQhQghURFhrM9LZFfV/CcY1boGiHWGkRojqyyajQS6ECGmtCiVypY+2nqH5/X6Wlc/hakxssqiCUmgCxFiSovGl9N9uWZ+3S6TgS7MRwJdiBCzIjOO5OiIefWj9w27ae0dkf5zk5JAFyLE2GyKTUtSeLV27oF+zDW+7Zy00M1JAl2IEHRhbgKtvSM09wzN6XU1bTJk0cwk0IUIQSW5iQBzXgagqrWPCLtN1kE3qQUFulLqu0qpSqXUQaXUDqVUgq8KE0L4T3FmHBFhNsrr57ZQ11vNvRSlxxBul7agGS30X+VZYJXWejVQBdy38JKEEP4WEWbjgqz4ObfQK5r7WJ4R56eqxEItKNC11s9orccm/roHyF54SUKIQCjJSeBQUw+jY95ZPd/VN0J7/wjFmbF+rkzMly8/N30UeNKHxxNC+FFJbiIjY14qW3pn9fzJ563IlBa6WZ030JVSzymlDk/x5+bTnvNlYAz4/QzHuVcpVaaUKnO5fLOvoRBi/kpyx295zbbbpaJ5PNCXS6CbVtj5nqC1vmqm7yul7gRuBK7UWusZjrMN2Aawfv36aZ8nhAiMzHgnGXFO9p7o5M5Need9fmVzH+lxDpKiI/xfnJiXhY5yuQ74V+AmrfXCNyoUQgSMUopNS5J5paYdr/f8bay3mnsplta5qS20D/1+IBZ4Vil1QCn1Sx/UJIQIkC1FqXQNujlycuZ+9NExL7Wufgl0kztvl8tMtNZLfFWIECLwNi8ZX6hrd42LC7Ljp31eVWsfbo9meYaMcDEzmR0gRAhLjXVQkBrN/rqZJxi9VtsBwIb8pECUJeZJAl2IEFeSk0h5fTczjGlgV7WLorQYMuMjA1iZmCsJdCFC3LrFCXQMjNLQOfVCXcNuD28c76S0KDXAlYm5kkAXIsSV5Iwv1LWvfuqNo/ee6GRkzEvp0pRAliXmQQJdiBC3LCOW+MhwXq3pmPL7u6vbibDb2Cj956YngS5EiLPbFJcuSWF3dfuU/ei7qlysz0skKmJBg+JEAEigCyEoLUqhpXeY6okNLCadaB+gsqWPy5dJ/3kwkEAXQnDF8jSUgscPNp/x+I7yJpSCd61ZZFBlYi4k0IUQpMc52VyYwvbyxlPdLiNjHv68r5FNhckyXDFISKALIQDYui6Lhs4hnq9sA+B3e+pp6h7i3i2FBlcmZksCXQgBjHer5KdE860nK2npGeanz1dTWpTCZUul/zxYSKALIQAIt9v4yrtWcKx9gHd8/0V6htx86frlRpcl5kDGIQkhTrliWRoP3bmeR/c1srkwhZWLpl+wS5iPBLoQ4gyXL0vj8mVpRpch5kG6XIQQwiIk0IUQwiIk0IUQwiIk0IUQwiIk0IUQwiIk0IUQwiIk0IUQwiIk0IUQwiLUTBvD+u1NlXIBdfN8eQrQ7sNyAiHYag62eiH4ag62eiH4ag62euH8NS/WWk+7uI4hgb4QSqkyrfV6o+uYi2CrOdjqheCrOdjqheCrOdjqhYXXLF0uQghhERLoQghhEcEY6NuMLmAegq3mYKsXgq/mYKsXgq/mYKsXFlhz0PWhCyGEmFowttCFEEJMQQJdCCEsIqgCXSl1nVLqqFKqRin1JaPrmYpS6oRS6pBS6oBSqmzisSSl1LNKqeqJ/yYaXOOvlFJtSqnDpz02bY1KqfsmzvlRpdS1Jqn3q0qpponzfEApdYOJ6s1RSr2glKpQSh1RSn1m4nEzn+PpajbleVZKOZVSbyil3pyo9z8mHjfzOZ6uZt+dY611UPwB7EAtUABEAG8CK4yua4o6TwApZz32HeBLE19/Cfi2wTVuAdYBh89XI7Bi4lw7gPyJfwO7Cer9KvB/pniuGerNBNZNfB0LVE3UZeZzPF3NpjzPgAJiJr4OB14HLjb5OZ6uZp+d42BqoW8AarTWx7TWo8AfgZsNrmm2bgYenvj6YeAWA2tBa70L6Dzr4elqvBn4o9Z6RGt9HKhh/N8iYKapdzpmqLdZa71/4us+oALIwtzneLqap2NozXpc/8Rfwyf+aMx9jqereTpzrjmYAj0LaDjt743M/ANnFA08o5Tap5S6d+KxdK11M4xfOIAZN2ycrkYzn/d/VkodnOiSmfxobap6lVJ5QAnjrbGgOMdn1QwmPc9KKbtS6gDQBjyrtTb9OZ6mZvDROQ6mQFdTPGbGMZebtdbrgOuBTymlthhd0AKZ9bz/AigE1gLNwPcnHjdNvUqpGOAvwGe11r0zPXWKx8xSs2nPs9bao7VeC2QDG5RSq2Z4uuH1wrQ1++wcB1OgNwI5p/09GzhpUC3T0lqfnPhvG7CD8Y9IrUqpTICJ/7YZV+G0pqvRlOdda906cXF4gQd5+6OoKepVSoUzHoy/11pvn3jY1Od4qprNfp4BtNbdwIvAdZj8HE86vWZfnuNgCvS9QJFSKl8pFQHcDjxmcE1nUEpFK6ViJ78GrgEOM17nnRNPuxP4mzEVzmi6Gh8DbldKOZRS+UAR8IYB9Z1h8qKdcCvj5xlMUK9SSgEPARVa6x+c9i3TnuPpajbreVZKpSqlEia+jgSuAiox9zmesmafnuNA3uX1wV3iGxi/+14LfNnoeqaor4Dxu9JvAkcmawSSgZ1A9cR/kwyu8w+Mf7RzM94KuHumGoEvT5zzo8D1Jqn3t8Ah4ODED36mieq9lPGPxgeBAxN/bjD5OZ6uZlOeZ2A1UD5R12HgKxOPm/kcT1ezz86xTP0XQgiLCKYuFyGEEDOQQBdCCIuQQBdCCIuQQBdCCIuQQBdCCIuQQBdCCIuQQBdCCIv4/yVj/bmmVLHcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_dataset[0].shape)\n",
    "plt.plot(train_dataset[15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('Latent_wafer.pkl', 'wb')\n",
    "# dump information to that file\n",
    "pickle.dump(latent_info, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
